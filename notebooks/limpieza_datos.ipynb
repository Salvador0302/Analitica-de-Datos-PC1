{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "994a0e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando el diagnóstico inicial del archivo CSV (lectura completa)...\n",
      "Archivo: ..\\denuncias_2020_2025.csv\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '..\\\\denuncias_2020_2025.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Leer TODO el CSV en un DataFrame\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mFILE_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# separador fijo\u001b[39;49;00m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_bad_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwarn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mna_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m  \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNA\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mN/A\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mna\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn/a\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNULL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnull\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSin dato\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSIN DATO\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Normalizar blancos a NaN\u001b[39;00m\n\u001b[0;32m     26\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*$\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mnan, regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\SALVA\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SALVA\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\SALVA\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SALVA\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\SALVA\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '..\\\\denuncias_2020_2025.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "# Script 1: Diagnóstico inicial\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# --- Configuración ---\n",
    "FILE_PATH = r\"..\\denuncias_2020_2025.csv\"\n",
    "\n",
    "# --- Inicio del Script ---\n",
    "print(\"Iniciando el diagnóstico inicial del archivo CSV (lectura completa)...\")\n",
    "print(f\"Archivo: {FILE_PATH}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Leer TODO el CSV en un DataFrame\n",
    "df = pd.read_csv(\n",
    "    FILE_PATH,\n",
    "    sep=\",\",                # separador fijo\n",
    "    on_bad_lines=\"warn\",\n",
    "    low_memory=False,\n",
    "    na_values=[\"\", \" \", \"  \", \"NA\", \"N/A\", \"na\", \"n/a\", \"NULL\", \"null\", \"Sin dato\", \"SIN DATO\"]\n",
    ")\n",
    "\n",
    "# Normalizar blancos a NaN\n",
    "df = df.replace(r\"^\\s*$\", np.nan, regex=True)\n",
    "\n",
    "# --- 1) Información General y Tipos de Datos ---\n",
    "print(\"## 1) Información General y Tipos de Datos\")\n",
    "print(f\"* Filas totales: {len(df):,}\")\n",
    "print(f\"* Columnas: {df.shape[1]}\")\n",
    "mem_mb = df.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f\"* Uso de memoria aprox.: {mem_mb:.2f} MB\")\n",
    "\n",
    "info_df = pd.DataFrame({\n",
    "    \"Columna\": df.columns,\n",
    "    \"Valores No Nulos\": df.count().values,\n",
    "    \"Tipo de Dato\": [str(t) for t in df.dtypes.values]\n",
    "})\n",
    "\n",
    "# --- 2) Resumen de Valores Faltantes ---\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "missing_df_sorted = (\n",
    "    pd.DataFrame({\n",
    "        \"Columna\": df.columns,\n",
    "        \"Valores Faltantes\": missing_values.values,\n",
    "        \"% Faltante\": missing_percentage.values\n",
    "    })\n",
    "    .query(\"`Valores Faltantes` > 0\")\n",
    "    .sort_values(by=\"% Faltante\", ascending=False)\n",
    ")\n",
    "\n",
    "# --- 3) Estadísticas Descriptivas (Numéricas) ---\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "desc_num = df[num_cols].describe().T if num_cols else pd.DataFrame()\n",
    "\n",
    "# --- 4) Estadísticas Descriptivas (Categóricas) ---\n",
    "obj_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "desc_cat = df[obj_cols].describe().T if obj_cols else pd.DataFrame()\n",
    "\n",
    "# --- 5) Distribución para Categorías Clave (Top 10) ---\n",
    "key_cols = [\"departamento_hecho\", \"tipo_hecho\", \"turno_hecho\", \"ano_hecho\", \"mes_hecho\"]\n",
    "distribuciones = {}\n",
    "for col in key_cols:\n",
    "    if col in df.columns:\n",
    "        distribuciones[col] = (df[col].value_counts(normalize=True, dropna=False).head(10) * 100).round(2)\n",
    "\n",
    "print(\"\\nDiagnóstico listo. Variables útiles en memoria:\")\n",
    "print(\" - df                      → DataFrame completo para seguir trabajando\")\n",
    "print(\" - info_df                 → tabla con no nulos y tipos\")\n",
    "print(\" - missing_df_sorted       → columnas con faltantes (ordenadas por %)\")\n",
    "print(\" - desc_num / desc_cat     → describe() numérico y categórico\")\n",
    "print(\" - distribuciones[col]     → top-10 % para cada columna en key_cols presente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50ac1b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Vista preliminar de los datos:\n",
      "      fecha_hora_hecho  mes_hecho  dia_hecho departamento_hecho  \\\n",
      "0  2020-03-27 05:10:00          3         27        LA LIBERTAD   \n",
      "1  2020-03-28 05:10:00          3         28          CAJAMARCA   \n",
      "2  2020-03-28 05:00:00          3         28        LA LIBERTAD   \n",
      "\n",
      "  provincia_hecho distrito_hecho               tipo_hecho  id_tipo_hecho  \\\n",
      "0          ASCOPE         PAIJAN  INTERVENCION POLICIALES            602   \n",
      "1            JAEN           JAEN  INTERVENCION POLICIALES            602   \n",
      "2        TRUJILLO       TRUJILLO  INTERVENCION POLICIALES            602   \n",
      "\n",
      "                materia_hecho  id_materia_hecho  ...  \\\n",
      "0  HECHOS DE INTERES POLICIAL                 6  ...   \n",
      "1  HECHOS DE INTERES POLICIAL                 6  ...   \n",
      "2  HECHOS DE INTERES POLICIAL                 6  ...   \n",
      "\n",
      "   fecha_hora_registro_hecho  barrio  comisaria  departamento  provincia  \\\n",
      "0              1585409365000     NaN        NaN           NaN        NaN   \n",
      "1              1586198863000     NaN        NaN           NaN        NaN   \n",
      "2              1585430285000     NaN        NaN           NaN        NaN   \n",
      "\n",
      "   distrito  indice_priorizacion fecha_inaguracion fecha_hecho  hora_hecho  \n",
      "0       NaN                  NaN               NaN  27/03/2020    05:10:00  \n",
      "1       NaN                  NaN               NaN  28/03/2020    05:10:00  \n",
      "2       NaN                  NaN               NaN  28/03/2020    05:00:00  \n",
      "\n",
      "[3 rows x 60 columns]\n",
      "\n",
      "### Columnas con más valores faltantes:\n",
      "                Columna  Valores Faltantes  % Faltante\n",
      "45        tipologias_ia            7425530  100.000000\n",
      "22         cuadra_hecho            7217259   97.195204\n",
      "53         departamento            7188959   96.814086\n",
      "55             distrito            7188959   96.814086\n",
      "54            provincia            7188959   96.814086\n",
      "56  indice_priorizacion            7184904   96.759477\n",
      "52            comisaria            7184904   96.759477\n",
      "57    fecha_inaguracion            7184904   96.759477\n",
      "51               barrio            7184904   96.759477\n",
      "21       tipo_via_hecho             152262    2.050520\n",
      "\n",
      "### Resumen estadístico de variables numéricas:\n",
      "                      count        mean         std         min        25%  \\\n",
      "mes_hecho         7425530.0    6.227308    3.470723    1.000000    3.00000   \n",
      "dia_hecho         7425530.0   15.652627    8.763031    1.000000    8.00000   \n",
      "id_tipo_hecho     7425530.0  364.117953  224.754865  101.000000  105.00000   \n",
      "id_materia_hecho  7425530.0    3.586809    2.275209    1.000000    1.00000   \n",
      "lat               7425530.0  -11.506161    3.336873  -18.348545  -13.23901   \n",
      "\n",
      "                         50%         75%         max  \n",
      "mes_hecho           6.000000    9.000000   12.000000  \n",
      "dia_hecho          16.000000   23.000000   31.000000  \n",
      "id_tipo_hecho     504.000000  602.000000  714.000000  \n",
      "id_materia_hecho    5.000000    6.000000    7.000000  \n",
      "lat               -12.041914   -9.264183   -0.119684  \n",
      "\n",
      "### Distribución de 'tipo_hecho' (Top 10 categorías en %):\n",
      "tipo_hecho\n",
      "INTERVENCION POLICIALES                                        30.39\n",
      "PATRIMONIO (DELITO)                                            25.02\n",
      "LEY DE VIOLENCIA CONTRA LA MUJER Y GRUPOS VULNERABLES          23.93\n",
      "SEGURIDAD PUBLICA (DELITO)                                      4.65\n",
      "FALTAS                                                          4.52\n",
      "VIDA, EL CUERPO Y LA SALUD (DELITO)                             3.76\n",
      "LIBERTAD (DELITO)                                               3.06\n",
      "LEY 30096 DELITOS INFORMATICOS, MODIFICADA POR LA LEY 30171     1.88\n",
      "ADMINISTRACION PUBLICA (DELITO)                                 1.21\n",
      "FAMILIA (DELITO)                                                0.35\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Extras para mostrar resultados\n",
    "# ===============================\n",
    "\n",
    "# 1) Primeras filas de la data\n",
    "print(\"\\n### Vista preliminar de los datos:\")\n",
    "print(df.head(3))  # muestra las 3 primeras filas\n",
    "\n",
    "# 2) Top columnas con más faltantes\n",
    "print(\"\\n### Columnas con más valores faltantes:\")\n",
    "print(missing_df_sorted.head(10))\n",
    "\n",
    "# 3) Estadísticas numéricas básicas\n",
    "if not desc_num.empty:\n",
    "    print(\"\\n### Resumen estadístico de variables numéricas:\")\n",
    "    print(desc_num.head())\n",
    "\n",
    "# 4) Ejemplo de distribución categórica\n",
    "ejemplo_col = \"tipo_hecho\"\n",
    "if ejemplo_col in distribuciones:\n",
    "    print(f\"\\n### Distribución de '{ejemplo_col}' (Top 10 categorías en %):\")\n",
    "    print(distribuciones[ejemplo_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d796f911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando Paso 2: Manejo de valores faltantes (Eliminación de columnas).\n",
      "Columnas a eliminar: ['tipologias_ia', 'cuadra_hecho', 'barrio', 'comisaria', 'departamento', 'provincia', 'distrito', 'indice_priorizacion', 'fecha_inaguracion']\n",
      "================================================================================\n",
      "Proceso completado.\n",
      "Tiempo total de procesamiento: 1.86 segundos.\n",
      "El DataFrame ahora tiene 51 columnas y 7,425,530 filas.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "# Script 2: Manejo de valores faltantes\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# --- Configuración ---\n",
    "COLUMNS_TO_DROP = [\n",
    "    'tipologias_ia',\n",
    "    'cuadra_hecho',\n",
    "    'barrio',\n",
    "    'comisaria',\n",
    "    'departamento',\n",
    "    'provincia',\n",
    "    'distrito',\n",
    "    'indice_priorizacion',\n",
    "    'fecha_inaguracion'\n",
    "]\n",
    "\n",
    "print(\"Iniciando Paso 2: Manejo de valores faltantes (Eliminación de columnas).\")\n",
    "print(f\"Columnas a eliminar: {COLUMNS_TO_DROP}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Usar inplace=True para modificar df directamente\n",
    "df.drop(columns=COLUMNS_TO_DROP, inplace=True, errors='ignore')\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Proceso completado.\")\n",
    "print(f\"Tiempo total de procesamiento: {end_time - start_time:.2f} segundos.\")\n",
    "print(f\"El DataFrame ahora tiene {df.shape[1]} columnas y {df.shape[0]:,} filas.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91a89d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando Paso 3: Transformación de Columnas (Fechas) sobre df en memoria.\n",
      "Columnas a eliminar: ['fecha_hecho', 'hora_hecho']\n",
      "================================================================================\n",
      "Proceso completado.\n",
      "Tiempo total de procesamiento: 3.37 segundos.\n",
      "Dimensiones actuales de df: 7,425,530 filas x 49 columnas.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "# Script 3: Transformar columnas\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "\n",
    "COLUMNS_TO_DROP = [\n",
    "    'fecha_hecho',\n",
    "    'hora_hecho'\n",
    "]\n",
    "\n",
    "print(\"Iniciando Paso 3: Transformación de Columnas (Fechas) sobre df en memoria.\")\n",
    "print(f\"Columnas a eliminar: {COLUMNS_TO_DROP}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# 1) Convertir 'fecha_hora_hecho' a datetime (coerce → valores inválidos pasan a NaT)\n",
    "if 'fecha_hora_hecho' in df.columns:\n",
    "    df['fecha_hora_hecho'] = pd.to_datetime(df['fecha_hora_hecho'], errors='coerce')\n",
    "else:\n",
    "    print(\"Aviso: la columna 'fecha_hora_hecho' no existe en df; no se aplicó la conversión.\")\n",
    "\n",
    "# 2) Eliminar columnas de fecha/hora redundantes\n",
    "df.drop(columns=COLUMNS_TO_DROP, inplace=True, errors='ignore')\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Proceso completado.\")\n",
    "print(f\"Tiempo total de procesamiento: {end_time - start_time:.2f} segundos.\")\n",
    "print(f\"Dimensiones actuales de df: {df.shape[0]:,} filas x {df.shape[1]} columnas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d2d8b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando Paso 4: Optimización de Tipos de Datos (Numéricos) sobre df en memoria.\n",
      "================================================================================\n",
      "Uso de memoria ANTES de optimizar: 11663.10 MB\n",
      "\n",
      "Proceso completado.\n",
      "Tiempo total de procesamiento: 0.12 segundos.\n",
      "Dimensiones actuales de df: 7,425,530 filas x 49 columnas.\n",
      "Uso de memoria DESPUÉS de optimizar: 11167.39 MB\n",
      "Reducción de memoria: 495.71 MB (4.25%)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "# Script 4: Optimizar tipos\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "\n",
    "DTYPE_MAPPING = {\n",
    "    'mes_hecho': 'int8',\n",
    "    'dia_hecho': 'int8',\n",
    "    'id_materia_hecho': 'int8',\n",
    "    'id_dpto_hecho': 'int8',\n",
    "    'solo_denuncia': 'int8',\n",
    "    'estado': 'int8',\n",
    "    'ano_hecho': 'int16',\n",
    "    'id_tipo_hecho': 'int16',\n",
    "    'lat': 'float32',\n",
    "    'lon': 'float32',\n",
    "    'lat_hecho': 'float32',\n",
    "    'long_hecho': 'float32',\n",
    "}\n",
    "\n",
    "print(\"Iniciando Paso 4: Optimización de Tipos de Datos (Numéricos) sobre df en memoria.\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# --- Medir memoria antes ---\n",
    "mem_before = df.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f\"Uso de memoria ANTES de optimizar: {mem_before:.2f} MB\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Aplicar conversiones\n",
    "for col, dtype in DTYPE_MAPPING.items():\n",
    "    if col in df.columns:\n",
    "        try:\n",
    "            df[col] = df[col].astype(dtype)\n",
    "        except Exception as e:\n",
    "            print(f\"  - Advertencia: No se pudo convertir la columna '{col}'. Error: {e}\")\n",
    "    else:\n",
    "        print(f\"  - Aviso: La columna '{col}' no existe en df; se omite la conversión.\")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# --- Medir memoria después ---\n",
    "mem_after = df.memory_usage(deep=True).sum() / 1024**2\n",
    "reduction_pct = ((mem_before - mem_after) / mem_before * 100) if mem_before > 0 else 0\n",
    "\n",
    "# Resumen\n",
    "print(\"\\nProceso completado.\")\n",
    "print(f\"Tiempo total de procesamiento: {end_time - start_time:.2f} segundos.\")\n",
    "print(f\"Dimensiones actuales de df: {df.shape[0]:,} filas x {df.shape[1]} columnas.\")\n",
    "print(f\"Uso de memoria DESPUÉS de optimizar: {mem_after:.2f} MB\")\n",
    "print(f\"Reducción de memoria: {mem_before - mem_after:.2f} MB ({reduction_pct:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a27d1a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando Paso 5: Verificación de Tipos de Datos sobre df en memoria.\n",
      "Esto puede tardar algunos segundos con ~7M filas…\n",
      "================================================================================\n",
      "\n",
      "Proceso completado en 2.72 segundos. Total de filas analizadas: 7,425,530\n",
      "\n",
      "--- Rangos de Columnas Numéricas ---\n",
      "Columna                   Min Real             Max Real            \n",
      "----------------------------------------------------------------------\n",
      "mes_hecho                 1.0                  12.0                \n",
      "dia_hecho                 1.0                  31.0                \n",
      "id_tipo_hecho             101.0                714.0               \n",
      "id_materia_hecho          1.0                  7.0                 \n",
      "ano_hecho                 2020.0               2025.0              \n",
      "id_dpto_hecho             1.0                  25.0                \n",
      "solo_denuncia             0.0                  1.0                 \n",
      "estado                    1.0                  3.0                 \n",
      "lat                       -18.348545           -0.1196843          \n",
      "lon                       -81.324486           -68.81834           \n",
      "lat_hecho                 -18.348545           -0.1196843          \n",
      "long_hecho                -81.324486           -68.81834           \n",
      "\n",
      "--- Conteo de Valores Únicos (Categóricas) ---\n",
      "Columna                   Valores Únicos \n",
      "---------------------------------------------\n",
      "departamento_hecho        25             \n",
      "provincia_hecho           196            \n",
      "distrito_hecho            1732           \n",
      "tipo_hecho                53             \n",
      "materia_hecho             5              \n",
      "turno_hecho               4              \n",
      "es_delito_x               5              \n",
      "macroregpol_hecho         23             \n",
      "regionpol_hecho           29             \n",
      "estado_coord              4              \n",
      "\n",
      "================================================================================\n",
      "Análisis completado. Usar estos rangos y cardinalidades para validar dtypes/consistencias.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "# Script 5: Verificar tipos\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# --- Configuración ---\n",
    "NUMERIC_COLS = [\n",
    "    'mes_hecho', 'dia_hecho', 'id_tipo_hecho', 'id_materia_hecho',\n",
    "    'ano_hecho', 'id_dpto_hecho', 'solo_denuncia', 'estado',\n",
    "    'lat', 'lon', 'lat_hecho', 'long_hecho'\n",
    "]\n",
    "CATEGORICAL_COLS = [\n",
    "    'departamento_hecho', 'provincia_hecho', 'distrito_hecho',\n",
    "    'tipo_hecho', 'materia_hecho', 'turno_hecho', 'es_delito_x',\n",
    "    'macroregpol_hecho', 'regionpol_hecho', 'estado_coord'\n",
    "]\n",
    "\n",
    "print(\"Iniciando Paso 5: Verificación de Tipos de Datos sobre df en memoria.\")\n",
    "print(\"Esto puede tardar algunos segundos con ~7M filas…\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Filtra a solo columnas existentes para evitar warnings innecesarios\n",
    "num_exist = [c for c in NUMERIC_COLS if c in df.columns]\n",
    "cat_exist = [c for c in CATEGORICAL_COLS if c in df.columns]\n",
    "\n",
    "# --- Rangos (min/max) para numéricas ---\n",
    "if num_exist:\n",
    "    # .agg(['min','max']) es vectorizado y muy eficiente\n",
    "    num_summary = df[num_exist].agg(['min', 'max']).T\n",
    "    num_summary.columns = ['min_real', 'max_real']\n",
    "else:\n",
    "    num_summary = pd.DataFrame(columns=['min_real', 'max_real'])\n",
    "\n",
    "# --- Conteo de valores únicos para categóricas ---\n",
    "if cat_exist:\n",
    "    cat_nunique = df[cat_exist].nunique(dropna=True)\n",
    "else:\n",
    "    cat_nunique = pd.Series(dtype='Int64')\n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "\n",
    "# --- Reporte en consola (mismo estilo que tu script original) ---\n",
    "print(f\"\\nProceso completado en {duration:.2f} segundos. Total de filas analizadas: {len(df):,}\")\n",
    "print(\"\\n--- Rangos de Columnas Numéricas ---\")\n",
    "print(f\"{'Columna':<25} {'Min Real':<20} {'Max Real':<20}\")\n",
    "print(\"-\" * 70)\n",
    "for col in num_summary.index:\n",
    "    mn = num_summary.loc[col, 'min_real']\n",
    "    mx = num_summary.loc[col, 'max_real']\n",
    "    print(f\"{col:<25} {str(mn):<20} {str(mx):<20}\")\n",
    "\n",
    "print(\"\\n--- Conteo de Valores Únicos (Categóricas) ---\")\n",
    "print(f\"{'Columna':<25} {'Valores Únicos':<15}\")\n",
    "print(\"-\" * 45)\n",
    "for col in cat_nunique.index:\n",
    "    print(f\"{col:<25} {int(cat_nunique[col]):<15}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Análisis completado. Usar estos rangos y cardinalidades para validar dtypes/consistencias.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "87aabb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Top 5 columnas categóricas con más valores únicos:\n",
      "distrito_hecho        1732\n",
      "provincia_hecho        196\n",
      "tipo_hecho              53\n",
      "regionpol_hecho         29\n",
      "departamento_hecho      25\n",
      "dtype: int64\n",
      "\n",
      "### Distribución de 'tipo_hecho' (Top 10 categorías):\n",
      "tipo_hecho\n",
      "INTERVENCION POLICIALES                                        2256255\n",
      "PATRIMONIO (DELITO)                                            1857941\n",
      "LEY DE VIOLENCIA CONTRA LA MUJER Y GRUPOS VULNERABLES          1777205\n",
      "SEGURIDAD PUBLICA (DELITO)                                      345496\n",
      "FALTAS                                                          335756\n",
      "VIDA, EL CUERPO Y LA SALUD (DELITO)                             279227\n",
      "LIBERTAD (DELITO)                                               227486\n",
      "LEY 30096 DELITOS INFORMATICOS, MODIFICADA POR LA LEY 30171     139735\n",
      "ADMINISTRACION PUBLICA (DELITO)                                  90046\n",
      "FAMILIA (DELITO)                                                 25785\n",
      "Name: count, dtype: int64\n",
      "\n",
      "### Percentiles 0.5% y 99.5% de variables numéricas:\n",
      "                        0.005        0.995\n",
      "mes_hecho            1.000000    12.000000\n",
      "dia_hecho            1.000000    31.000000\n",
      "id_tipo_hecho      101.000000   602.000000\n",
      "id_materia_hecho     1.000000     6.000000\n",
      "ano_hecho         2020.000000  2025.000000\n",
      "id_dpto_hecho        1.000000    25.000000\n",
      "solo_denuncia        0.000000     1.000000\n",
      "estado               1.000000     3.000000\n",
      "lat                -18.009757    -3.579531\n",
      "lon                -80.897484   -69.857445\n",
      "lat_hecho          -18.009757    -3.579531\n",
      "long_hecho         -80.897484   -69.857445\n"
     ]
    }
   ],
   "source": [
    "# --- Extras---\n",
    "print(\"\\n### Top 5 columnas categóricas con más valores únicos:\")\n",
    "print(cat_nunique.sort_values(ascending=False).head())\n",
    "\n",
    "ejemplo_col = \"tipo_hecho\"\n",
    "if ejemplo_col in df.columns:\n",
    "    print(f\"\\n### Distribución de '{ejemplo_col}' (Top 10 categorías):\")\n",
    "    print(df[ejemplo_col].value_counts().head(10))\n",
    "\n",
    "if num_exist:\n",
    "    print(\"\\n### Percentiles 0.5% y 99.5% de variables numéricas:\")\n",
    "    print(df[num_exist].quantile([0.005, 0.995]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c355e733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando Paso 5: Renombrar y Estandarizar Nombres de Columnas (sobre df en memoria).\n",
      "================================================================================\n",
      "Se aplicarán los siguientes cambios en los nombres:\n",
      "(No se encontraron columnas que necesiten cambios)\n",
      "\n",
      "Proceso completado.\n",
      "Tiempo total de procesamiento: 0.00 segundos.\n",
      "Dimensiones actuales de df: 7,425,530 filas x 49 columnas.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import unicodedata\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "# Script 6: Renombrar columnas\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def normalize_column_name(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Normaliza un nombre de columna a formato snake_case.\n",
    "    Ej: 'Año Hecho' -> 'ano_hecho'\n",
    "    \"\"\"\n",
    "    # Transliterar acentos/ñ\n",
    "    nfkd = unicodedata.normalize('NFKD', str(name))\n",
    "    name = ''.join(c for c in nfkd if not unicodedata.combining(c))\n",
    "    # Minúsculas\n",
    "    name = name.lower()\n",
    "    # Reemplazar no alfanuméricos por '_'\n",
    "    name = re.sub(r'[^a-z0-9]+', '_', name)\n",
    "    # Quitar '_' inicial/final\n",
    "    name = name.strip('_')\n",
    "    return name\n",
    "\n",
    "print(\"Iniciando Paso 5: Renombrar y Estandarizar Nombres de Columnas (sobre df en memoria).\")\n",
    "print(\"=\"*80)\n",
    "start_time = time.time()\n",
    "\n",
    "# Construir mapeo old -> new a partir de las columnas actuales del df\n",
    "new_columns = {col: normalize_column_name(col) for col in df.columns}\n",
    "\n",
    "# Mostrar cambios propuestos\n",
    "print(\"Se aplicarán los siguientes cambios en los nombres:\")\n",
    "changed = False\n",
    "for old, new in new_columns.items():\n",
    "    if old != new:\n",
    "        print(f\"- '{old}' -> '{new}'\")\n",
    "        changed = True\n",
    "if not changed:\n",
    "    print(\"(No se encontraron columnas que necesiten cambios)\")\n",
    "\n",
    "# Aplicar renombrado en el DataFrame en memoria\n",
    "df.rename(columns=new_columns, inplace=True)\n",
    "\n",
    "# Aviso si quedaron nombres duplicados tras la normalización\n",
    "dup_cols = df.columns[df.columns.duplicated()].unique().tolist()\n",
    "if dup_cols:\n",
    "    print(\"\\nADVERTENCIA: Se detectaron nombres de columnas duplicados después de normalizar:\")\n",
    "    for c in dup_cols:\n",
    "        print(f\"  - '{c}' aparece {sum(df.columns == c)} veces\")\n",
    "    print(\"Sugerencia: resolver manualmente estas colisiones antes de continuar.\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"\\nProceso completado.\")\n",
    "print(f\"Tiempo total de procesamiento: {end_time - start_time:.2f} segundos.\")\n",
    "print(f\"Dimensiones actuales de df: {df.shape[0]:,} filas x {df.shape[1]} columnas.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3257a143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando Verificación de Unicidad para la columna 'objectid' (sobre df en memoria).\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "  Reporte de Verificación de Unicidad\n",
      "================================================================================\n",
      "Proceso completado en 0.39 segundos.\n",
      "Columna analizada: 'objectid'\n",
      "Total de filas analizadas: 7,425,530\n",
      "Valores únicos encontrados: 7,425,530\n",
      "\n",
      "--- Conclusión ---\n",
      "(OK) ¡Confirmado! La columna es un identificador único. No hay duplicados.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "# Script 7: Verificar unicidad\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# --- Configuración ---\n",
    "COLUMN_TO_CHECK = 'objectid'\n",
    "\n",
    "print(f\"Iniciando Verificación de Unicidad para la columna '{COLUMN_TO_CHECK}' (sobre df en memoria).\")\n",
    "print(\"=\"*80)\n",
    "start_time = time.time()\n",
    "\n",
    "if COLUMN_TO_CHECK not in df.columns:\n",
    "    raise KeyError(f\"La columna '{COLUMN_TO_CHECK}' no existe en df. Columnas disponibles: {list(df.columns)[:10]}...\")\n",
    "\n",
    "total_rows = len(df)\n",
    "# Contar únicos a nivel global; dropna=False cuenta NaN como una categoría\n",
    "unique_count = df[COLUMN_TO_CHECK].nunique(dropna=False)\n",
    "\n",
    "duration = time.time() - start_time\n",
    "\n",
    "# --- Reporte ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"  Reporte de Verificación de Unicidad\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Proceso completado en {duration:.2f} segundos.\")\n",
    "print(f\"Columna analizada: '{COLUMN_TO_CHECK}'\")\n",
    "print(f\"Total de filas analizadas: {total_rows:,}\")\n",
    "print(f\"Valores únicos encontrados: {unique_count:,}\")\n",
    "\n",
    "print(\"\\n--- Conclusión ---\")\n",
    "if total_rows == unique_count:\n",
    "    print(\"(OK) ¡Confirmado! La columna es un identificador único. No hay duplicados.\")\n",
    "else:\n",
    "    calculated_duplicates = total_rows - unique_count\n",
    "    print(f\"(ERROR) ¡Atención! La columna NO es un identificador único.\")\n",
    "    print(f\"   Se encontraron {calculated_duplicates:,} filas duplicadas basadas en '{COLUMN_TO_CHECK}'.\")\n",
    "\n",
    "    # (Opcional) muestra una vista rápida de IDs duplicados y sus conteos\n",
    "    dup_counts = df[COLUMN_TO_CHECK].value_counts(dropna=False)\n",
    "    top_dups = dup_counts[dup_counts > 1].head(10)\n",
    "    if not top_dups.empty:\n",
    "        print(\"\\nEjemplos de IDs duplicados (top 10):\")\n",
    "        for idx, cnt in top_dups.items():\n",
    "            print(f\"  - {idx}: {cnt} veces\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "67cb3746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando Verificación de Valores Únicos para Columnas Categóricas (sobre df en memoria).\n",
      "Columnas objetivo: ['departamento_hecho', 'provincia_hecho', 'distrito_hecho', 'tipo_hecho', 'materia_hecho', 'turno_hecho', 'es_delito_x', 'macroregpol_hecho', 'regionpol_hecho', 'estado_coord']\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "  Diccionario de Valores Únicos por Columna Categórica\n",
      "================================================================================\n",
      "Proceso completado en 4.47 segundos.\n",
      "\n",
      "--- Columna: 'departamento_hecho' (25 valores únicos) ---\n",
      "['AMAZONAS', 'ANCASH', 'APURIMAC', 'AREQUIPA', 'AYACUCHO', 'CAJAMARCA', 'CALLAO', 'CUSCO', 'HUANCAVELICA', 'HUANUCO'] ...\n",
      "\n",
      "--- Columna: 'provincia_hecho' (196 valores únicos) ---\n",
      "['ABANCAY', 'ACOBAMBA', 'ACOMAYO', 'AIJA', 'ALTO AMAZONAS', 'AMBO', 'ANDAHUAYLAS', 'ANGARAES', 'ANTA', 'ANTABAMBA'] ...\n",
      "\n",
      "--- Columna: 'distrito_hecho' (1732 valores únicos) ---\n",
      "['ABANCAY', 'ABELARDO PARDO LEZAMETA', 'ACARI', 'ACAS', 'ACCHA', 'ACCOMARCA', 'ACHAYA', 'ACHOMA', 'ACO', 'ACOBAMBA'] ...\n",
      "\n",
      "--- Columna: 'tipo_hecho' (53 valores únicos) ---\n",
      "['ADMINISTRACION PUBLICA (DELITO)', 'ADOLESCENTE INFRACTOR DE LA LEY PENAL', 'AMBIENTALES(DELITO)', 'CONFIANZA Y LA BUENA FE EN LOS NEGOCIOS (DELITO)', 'CONTRA LA DIGNIDAD HUMANA', 'CONTRAVENCION A LOS DERECHOS DE LOS NIÑOS Y ADOLESCENTES', 'CONTRAVENCIONES', 'DELITOS ADUANEROS', 'DELITOS TRIBUTARIOS', 'DENUNCIAS ESPECIALES'] ...\n",
      "\n",
      "--- Columna: 'materia_hecho' (5 valores únicos) ---\n",
      "['FUERO COMUN', 'HECHOS DE INTERES POLICIAL', 'LEYES ESPECIALES', 'NIÑOS Y ADOLESCENTES', 'POLICIAL']\n",
      "\n",
      "--- Columna: 'turno_hecho' (4 valores únicos) ---\n",
      "['madrugada', 'mañana', 'noche', 'tarde']\n",
      "\n",
      "--- Columna: 'es_delito_x' (5 valores únicos) ---\n",
      "['1.Delitos', '2.Faltas', '3. Niños y adolescentes', '4.Violencia contra la mujer e int', 'Otros']\n",
      "\n",
      "--- Columna: 'macroregpol_hecho' (23 valores únicos) ---\n",
      "['ANCASH', 'APURIMAC', 'AREQUIPA', 'AYACUCHO', 'CAJAMARCA', 'CALLAO', 'CUSCO', 'HUANUCO', 'ICA', 'JUNIN'] ...\n",
      "\n",
      "--- Columna: 'regionpol_hecho' (29 valores únicos) ---\n",
      "['AMAZONAS', 'ANCASH', 'APURIMAC', 'AREQUIPA', 'AYACUCHO', 'CAJAMARCA', 'CALLAO', 'CUSCO', 'HUANCAVELICA', 'HUANUCO'] ...\n",
      "\n",
      "--- Columna: 'estado_coord' (4 valores únicos) ---\n",
      "['CON COORDENADA', 'SIN COORDENADA', 'SIN COORDENADA XX', 'SIN COORDENADA YY']\n",
      "\n",
      "Nota: El diccionario completo quedó en la variable `final_uniques_dict` para uso posterior.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "# Script 8: Verificar categorías\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# --- Configuración ---\n",
    "CATEGORICAL_COLS = [\n",
    "    'departamento_hecho', 'provincia_hecho', 'distrito_hecho',\n",
    "    'tipo_hecho', 'materia_hecho', 'turno_hecho', 'es_delito_x',\n",
    "    'macroregpol_hecho', 'regionpol_hecho', 'estado_coord'\n",
    "]\n",
    "\n",
    "print(\"Iniciando Verificación de Valores Únicos para Columnas Categóricas (sobre df en memoria).\")\n",
    "print(\"Columnas objetivo:\", CATEGORICAL_COLS)\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Filtrar a las columnas que realmente existen en df\n",
    "cat_exist = [c for c in CATEGORICAL_COLS if c in df.columns]\n",
    "missing = [c for c in CATEGORICAL_COLS if c not in df.columns]\n",
    "if missing:\n",
    "    print(\"Aviso: estas columnas no existen en df y serán omitidas:\", missing)\n",
    "\n",
    "# Construir diccionario de valores únicos\n",
    "final_uniques_dict = {}\n",
    "for col in cat_exist:\n",
    "    # dropna() para ignorar NaN; sorted para salida consistente\n",
    "    uniques = df[col].dropna().unique()\n",
    "    try:\n",
    "        # Si viene como dtype 'category', .unique() ya es eficiente\n",
    "        uniques_list = sorted([str(x) for x in uniques])\n",
    "    except Exception:\n",
    "        # Fallback robusto\n",
    "        uniques_list = sorted(map(str, pd.Series(uniques).astype(str).tolist()))\n",
    "    final_uniques_dict[col] = uniques_list\n",
    "\n",
    "duration = time.time() - start_time\n",
    "\n",
    "# --- Reporte en consola (legible) ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"  Diccionario de Valores Únicos por Columna Categórica\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Proceso completado en {duration:.2f} segundos.\")\n",
    "for col, values in final_uniques_dict.items():\n",
    "    print(f\"\\n--- Columna: '{col}' ({len(values)} valores únicos) ---\")\n",
    "    if len(values) > 20:\n",
    "        print(values[:10], \"...\")\n",
    "    else:\n",
    "        print(values)\n",
    "\n",
    "print(\"\\nNota: El diccionario completo quedó en la variable `final_uniques_dict` para uso posterior.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fff1a079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando Codificación y Binning.\n",
      "================================================================================\n",
      "\n",
      "Proceso completado.\n",
      "Tiempo total de procesamiento: 4.14 segundos.\n",
      "Dimensiones actuales de df: 7,425,530 filas x 52 columnas.\n",
      "Variables nuevas/afectadas: 'estado_coord' (limpia), 'turno_hecho_cod', 'periodo_dia', 'tiene_coordenada'.\n",
      "Diccionario de datos disponible en la variable `data_dictionary`.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "# Script 9: Codificación y Binning\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "\n",
    "print(\"Iniciando Codificación y Binning.\")\n",
    "print(\"=\"*80)\n",
    "start_time = time.time()\n",
    "\n",
    "# --- 1) Limpieza para 'estado_coord' ---\n",
    "estado_coord_replace_map = {\n",
    "    'SIN COORDENADA XX': 'SIN COORDENADA',\n",
    "    'SIN COORDENADA YY': 'SIN COORDENADA'\n",
    "}\n",
    "\n",
    "# --- 2) Codificación Ordinal para 'turno_hecho' (asume lower-case) ---\n",
    "turno_hecho_encoding_map = {\n",
    "    'madrugada': 0,\n",
    "    'mañana': 1,\n",
    "    'tarde': 2,\n",
    "    'noche': 3\n",
    "}\n",
    "\n",
    "# --- 3) Binning para la hora del día desde 'fecha_hora_hecho' ---\n",
    "hour_bins   = [-1, 5, 11, 17, 23]\n",
    "hour_labels = ['Madrugada', 'Mañana', 'Tarde', 'Noche']\n",
    "\n",
    "# --- 4) Codificación binaria para 'tiene_coordenada' a partir de 'estado_coord' ---\n",
    "tiene_coordenada_map = {\n",
    "    'CON COORDENADA': 1,\n",
    "    'SIN COORDENADA': 0\n",
    "}\n",
    "\n",
    "# --- Diccionario de Datos para documentación (en memoria) ---\n",
    "data_dictionary = {\n",
    "    \"estado_coord_cleaning\": {\n",
    "        \"description\": \"Se unificaron valores en la columna 'estado_coord'.\",\n",
    "        \"mapping\": estado_coord_replace_map\n",
    "    },\n",
    "    \"turno_hecho_encoding\": {\n",
    "        \"description\": \"Codificación ordinal de la columna 'turno_hecho'.\",\n",
    "        \"mapping\": {\"0\": \"madrugada\", \"1\": \"mañana\", \"2\": \"tarde\", \"3\": \"noche\"}\n",
    "    },\n",
    "    \"periodo_dia_binning\": {\n",
    "        \"description\": \"Nueva columna creada agrupando la hora de 'fecha_hora_hecho'.\",\n",
    "        \"bins\": {\"0-5\": \"Madrugada\", \"6-11\": \"Mañana\", \"12-17\": \"Tarde\", \"18-23\": \"Noche\"}\n",
    "    },\n",
    "    \"tiene_coordenada_encoding\": {\n",
    "        \"description\": \"Codificación binaria (0/1) creada a partir de 'estado_coord' limpio.\",\n",
    "        \"mapping\": tiene_coordenada_map\n",
    "    }\n",
    "}\n",
    "\n",
    "# ---------- Aplicación sobre df ----------\n",
    "# 1) Limpiar 'estado_coord'\n",
    "if 'estado_coord' in df.columns:\n",
    "    df['estado_coord'] = df['estado_coord'].replace(estado_coord_replace_map)\n",
    "else:\n",
    "    print(\"Aviso: 'estado_coord' no existe en df; se omite su limpieza.\")\n",
    "\n",
    "# 2) Codificar 'turno_hecho' → 'turno_hecho_cod'\n",
    "if 'turno_hecho' in df.columns:\n",
    "    # Asegura lower-case si no lo hiciste antes\n",
    "    df['turno_hecho_cod'] = (\n",
    "        df['turno_hecho']\n",
    "        .astype('string')\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .map(turno_hecho_encoding_map)\n",
    "        .astype('Int8')  # permite NaN\n",
    "    )\n",
    "else:\n",
    "    print(\"Aviso: 'turno_hecho' no existe en df; no se creó 'turno_hecho_cod'.\")\n",
    "\n",
    "# 3) Binning por hora de 'fecha_hora_hecho' → 'periodo_dia'\n",
    "if 'fecha_hora_hecho' in df.columns:\n",
    "    # Asegura tipo datetime (Paso 3 ya lo hacía, esto es por si acaso)\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df['fecha_hora_hecho']):\n",
    "        df['fecha_hora_hecho'] = pd.to_datetime(df['fecha_hora_hecho'], errors='coerce')\n",
    "    horas = df['fecha_hora_hecho'].dt.hour  # NaT → NaN\n",
    "    df['periodo_dia'] = pd.cut(horas, bins=hour_bins, labels=hour_labels, right=True)\n",
    "else:\n",
    "    print(\"Aviso: 'fecha_hora_hecho' no existe en df; no se creó 'periodo_dia'.\")\n",
    "\n",
    "# 4) 'tiene_coordenada' (0/1) a partir de 'estado_coord' limpio\n",
    "if 'estado_coord' in df.columns:\n",
    "    df['tiene_coordenada'] = (\n",
    "        df['estado_coord']\n",
    "        .astype('string')\n",
    "        .str.strip()\n",
    "        .str.upper()\n",
    "        .map(tiene_coordenada_map)\n",
    "        .astype('Int8')  # permite NaN cuando no machea\n",
    "    )\n",
    "else:\n",
    "    print(\"Aviso: 'estado_coord' no existe en df; no se creó 'tiene_coordenada'.\")\n",
    "\n",
    "dur = time.time() - start_time\n",
    "print(\"\\nProceso completado.\")\n",
    "print(f\"Tiempo total de procesamiento: {dur:.2f} segundos.\")\n",
    "print(f\"Dimensiones actuales de df: {df.shape[0]:,} filas x {df.shape[1]} columnas.\")\n",
    "print(\"Variables nuevas/afectadas: 'estado_coord' (limpia), 'turno_hecho_cod', 'periodo_dia', 'tiene_coordenada'.\")\n",
    "print(\"Diccionario de datos disponible en la variable `data_dictionary`.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3884360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando Paso 10: Filtrado de Valores Atípicos (Geográficos) sobre df en memoria.\n",
      "Límites de Latitud: -18.4 a 0\n",
      "Límites de Longitud: -81.4 a -68.6\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Proceso de filtrado completado (en memoria).\n",
      "Total de filas procesadas: 7,425,530\n",
      "Filas eliminadas como outliers: 0\n",
      "Filas finales en df: 7,425,530\n",
      "Tiempo total de procesamiento: 2.94 segundos.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "# Script 10: Filtrar atípicos\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# --- Configuración ---\n",
    "LAT_MIN, LAT_MAX = -18.4, 0\n",
    "LON_MIN, LON_MAX = -81.4, -68.6\n",
    "\n",
    "print(\"Iniciando Paso 10: Filtrado de Valores Atípicos (Geográficos) sobre df en memoria.\")\n",
    "print(f\"Límites de Latitud: {LAT_MIN} a {LAT_MAX}\")\n",
    "print(f\"Límites de Longitud: {LON_MIN} a {LON_MAX}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time = time.time()\n",
    "rows_before = len(df)\n",
    "\n",
    "# --- Filtrado ---\n",
    "if 'lat' in df.columns and 'lon' in df.columns:\n",
    "    df = df[df['lat'].between(LAT_MIN, LAT_MAX) & df['lon'].between(LON_MIN, LON_MAX)]\n",
    "else:\n",
    "    raise KeyError(\"No se encontraron las columnas 'lat' y 'lon' en df.\")\n",
    "\n",
    "rows_after = len(df)\n",
    "outlier_rows = rows_before - rows_after\n",
    "\n",
    "duration = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Proceso de filtrado completado (en memoria).\")\n",
    "print(f\"Total de filas procesadas: {rows_before:,}\")\n",
    "print(f\"Filas eliminadas como outliers: {outlier_rows:,}\")\n",
    "print(f\"Filas finales en df: {rows_after:,}\")\n",
    "print(f\"Tiempo total de procesamiento: {duration:.2f} segundos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "926581cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No se encontraron outliers geográficos.\n",
      "\n",
      "### Rango de coordenadas tras el filtrado:\n",
      "Latitud: -18.34854507446289 a -0.11968430131673813\n",
      "Longitud: -81.3244857788086 a -68.8183364868164\n"
     ]
    }
   ],
   "source": [
    "# --- Extras ---\n",
    "if outlier_rows > 0:\n",
    "    print(\"\\n### Ejemplo de coordenadas fuera de rango (primeros 5):\")\n",
    "    # Filas descartadas (outliers)\n",
    "    mask_outliers = ~(\n",
    "        df['lat'].between(LAT_MIN, LAT_MAX) & df['lon'].between(LON_MIN, LON_MAX)\n",
    "    )\n",
    "    print(df.loc[mask_outliers, ['lat','lon']].head())\n",
    "else:\n",
    "    print(\"\\nNo se encontraron outliers geográficos.\")\n",
    "\n",
    "# Mostrar rangos finales\n",
    "print(\"\\n### Rango de coordenadas tras el filtrado:\")\n",
    "print(f\"Latitud: {df['lat'].min()} a {df['lat'].max()}\")\n",
    "print(f\"Longitud: {df['lon'].min()} a {df['lon'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8bfa4152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paso 10.1: Depuración final de columnas (en memoria).\n",
      "Columnas objetivo (tras aplicar banderas):\n",
      "['id_tipo_hecho', 'id_materia_hecho', 'id_subtipo_hecho', 'id_modalidad_hecho', 'id_dpto_hecho', 'id_prov_hecho', 'id_dist_hecho', 'lat', 'lon', 'ubigeo_cia_registro', 'ubigeo_cia_hecho', 'id_dgc', 'id_comisaria_registro', 'cod_uni_hecho', 'cod_cpnp_hecho', 'cod_macroregpol_hecho', 'cod_regpol_hecho', 'cod_divpol_divopus_hecho', 'objectid']\n",
      "================================================================================\n",
      "Eliminadas: 19 columnas\n",
      "df actual: 7,425,530 filas x 33 columnas\n",
      "Tiempo: 1.33s\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Paso 10.1: Depuración final de columnas\n",
    "# ==========================================\n",
    "import time\n",
    "\n",
    "# Si quieres conservar coordenadas/ID por pasos posteriores, activa estas banderas:\n",
    "KEEP_GEO = False        # True para conservar 'lat' y 'lon'\n",
    "KEEP_OBJECTID = False   # True para conservar 'objectid'\n",
    "\n",
    "# Lista original\n",
    "cols_to_drop_base = [\n",
    "    'id_tipo_hecho','id_materia_hecho','id_subtipo_hecho','id_modalidad_hecho',\n",
    "    'id_dpto_hecho','id_prov_hecho','id_dist_hecho','lat','lon',\n",
    "    'ubigeo_cia_registro','ubigeo_cia_hecho','id_dgc','id_comisaria_registro',\n",
    "    'cod_uni_hecho','cod_cpnp_hecho','cod_macroregpol_hecho','cod_regpol_hecho',\n",
    "    'cod_divpol_divopus_hecho','objectid'\n",
    "]\n",
    "\n",
    "# Ajuste según banderas\n",
    "cols_to_drop = cols_to_drop_base.copy()\n",
    "if KEEP_GEO:\n",
    "    for c in ['lat','lon']:\n",
    "        if c in cols_to_drop:\n",
    "            cols_to_drop.remove(c)\n",
    "if KEEP_OBJECTID and 'objectid' in cols_to_drop:\n",
    "    cols_to_drop.remove('objectid')\n",
    "\n",
    "print(\"Paso 10.1: Depuración final de columnas (en memoria).\")\n",
    "print(\"Columnas objetivo (tras aplicar banderas):\")\n",
    "print(cols_to_drop)\n",
    "print(\"=\"*80)\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# Eliminar solo las existentes\n",
    "existentes = [c for c in cols_to_drop if c in df.columns]\n",
    "faltantes  = [c for c in cols_to_drop if c not in df.columns]\n",
    "\n",
    "df.drop(columns=existentes, inplace=True, errors='ignore')\n",
    "\n",
    "print(f\"Eliminadas: {len(existentes)} columnas\")\n",
    "if faltantes:\n",
    "    print(f\"(Aviso) No se encontraron y se omitieron: {faltantes}\")\n",
    "print(f\"df actual: {df.shape[0]:,} filas x {df.shape[1]} columnas\")\n",
    "print(f\"Tiempo: {time.time() - t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1b25ce0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando Paso 10.2: Conversión de 'fecha_hora_hecho' a datetime y eliminación de columnas derivadas.\n",
      "================================================================================\n",
      "Columnas eliminadas: ['mes_hecho', 'dia_hecho', 'ano_hecho']\n",
      "\n",
      "Proceso completado.\n",
      "dtype de 'fecha_hora_hecho' -> datetime64[ns]\n",
      "Columnas totales en df -> 30\n",
      "Tiempo de procesamiento: 1.69 segundos.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Script 10.2: Ajuste de fecha_hora_hecho y limpieza de columnas derivadas\n",
    "# ==========================================\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Iniciando Paso 10.2: Conversión de 'fecha_hora_hecho' a datetime y eliminación de columnas derivadas.\")\n",
    "print(\"=\"*80)\n",
    "start_time = time.time()\n",
    "\n",
    "# 1) Convertir 'fecha_hora_hecho' a datetime (coerce para errores → NaT)\n",
    "if 'fecha_hora_hecho' in df.columns:\n",
    "    df['fecha_hora_hecho'] = pd.to_datetime(df['fecha_hora_hecho'], errors='coerce')\n",
    "else:\n",
    "    print(\"Aviso: 'fecha_hora_hecho' no existe en df.\")\n",
    "\n",
    "# 2) Eliminar columnas redundantes de fecha\n",
    "cols_to_remove = ['mes_hecho','dia_hecho','ano_hecho']\n",
    "existentes = [c for c in cols_to_remove if c in df.columns]\n",
    "\n",
    "if existentes:\n",
    "    df.drop(columns=existentes, inplace=True, errors='ignore')\n",
    "    print(f\"Columnas eliminadas: {existentes}\")\n",
    "else:\n",
    "    print(\"No se encontraron columnas ['mes_hecho','dia_hecho','ano_hecho'] para eliminar.\")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"\\nProceso completado.\")\n",
    "print(\"dtype de 'fecha_hora_hecho' ->\", df['fecha_hora_hecho'].dtype)\n",
    "print(f\"Columnas totales en df -> {df.shape[1]}\")\n",
    "print(f\"Tiempo de procesamiento: {end_time - start_time:.2f} segundos.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6299aff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando Paso 10.3: Eliminación de columnas administrativas irrelevantes.\n",
      "================================================================================\n",
      "Columnas eliminadas: ['fecha_hora_registro_hecho', 'turno_hecho_cod', 'estado', 'fuente', 'observacion', 'estado_coord', 'tiene_coordenada']\n",
      "\n",
      "Proceso completado.\n",
      "Columnas totales en df -> 23\n",
      "Tiempo de procesamiento: 1.07 segundos.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Script 10.3: Eliminación de columnas administrativas irrelevantes\n",
    "# ==========================================\n",
    "import time\n",
    "\n",
    "print(\"Iniciando Paso 10.3: Eliminación de columnas administrativas irrelevantes.\")\n",
    "print(\"=\"*80)\n",
    "start_time = time.time()\n",
    "\n",
    "cols_to_drop = [\n",
    "    \"fecha_hora_registro_hecho\", \"turno_hecho_cod\",\n",
    "    \"estado\", \"fuente\", \"observacion\",\n",
    "    \"estado_coord\", \"tiene_coordenada\"\n",
    "]\n",
    "\n",
    "# Eliminar solo las que existan en df\n",
    "existentes = [c for c in cols_to_drop if c in df.columns]\n",
    "faltantes = [c for c in cols_to_drop if c not in df.columns]\n",
    "\n",
    "if existentes:\n",
    "    df.drop(columns=existentes, inplace=True, errors='ignore')\n",
    "    print(f\"Columnas eliminadas: {existentes}\")\n",
    "else:\n",
    "    print(\"No se encontraron columnas administrativas para eliminar.\")\n",
    "\n",
    "if faltantes:\n",
    "    print(f\"(Aviso) Estas columnas no estaban en df y se omitieron: {faltantes}\")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"\\nProceso completado.\")\n",
    "print(f\"Columnas totales en df -> {df.shape[1]}\")\n",
    "print(f\"Tiempo de procesamiento: {end_time - start_time:.2f} segundos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "561e7729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando Paso 11: Guardado en un único archivo (denuncias_final.csv).\n",
      "================================================================================\n",
      "\n",
      "Proceso completado.\n",
      "Archivo guardado: C:\\Users\\diego\\OneDrive\\9NO CICLO\\ANALÍTICA DE DATOS\\PC1\\ENTREGABLES\\denuncias_final.csv (7,425,530 filas y 23 columnas)\n",
      "Tiempo total de procesamiento: 57.20 segundos.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Script 11: Guardar dataset completo en CSV\n",
    "# ==========================================\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Iniciando Paso 11: Guardado en un único archivo (denuncias_final.csv).\")\n",
    "print(\"=\"*80)\n",
    "start_time = time.time()\n",
    "\n",
    "# Validar columna de provincias\n",
    "if \"provincia_hecho\" not in df.columns:\n",
    "    raise KeyError(\"Error: La columna 'provincia_hecho' no se encuentra en df.\")\n",
    "\n",
    "# --- Guardar resultados ---\n",
    "OUTPUT_DIR = r\"C:\\Users\\diego\\OneDrive\\9NO CICLO\\ANALÍTICA DE DATOS\\PC1\\ENTREGABLES\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "OUTPUT_FILE = os.path.join(OUTPUT_DIR, \"denuncias_final.csv\")\n",
    "\n",
    "df.to_csv(OUTPUT_FILE, index=False, encoding=\"utf-8\")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"\\nProceso completado.\")\n",
    "print(f\"Archivo guardado: {OUTPUT_FILE} ({df.shape[0]:,} filas y {df.shape[1]:,} columnas)\")\n",
    "print(f\"Tiempo total de procesamiento: {end_time - start_time:.2f} segundos.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
