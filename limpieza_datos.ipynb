{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994a0e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando el diagnóstico inicial del archivo CSV (lectura completa)...\n",
      "Archivo: C:\\Users\\diego\\OneDrive\\9NO CICLO\\ANALÍTICA DE DATOS\\PC1\\Data\\denuncias_2020_2025.csv\n",
      "================================================================================\n",
      "## 1) Información General y Tipos de Datos\n",
      "* Filas totales: 7,425,530\n",
      "* Columnas: 60\n",
      "* Uso de memoria aprox.: 14474.90 MB\n",
      "\n",
      "Diagnóstico listo. Variables útiles en memoria:\n",
      " - df                      → DataFrame completo para seguir trabajando\n",
      " - info_df                 → tabla con no nulos y tipos\n",
      " - missing_df_sorted       → columnas con faltantes (ordenadas por %)\n",
      " - desc_num / desc_cat     → describe() numérico y categórico\n",
      " - distribuciones[col]     → top-10 % para cada columna en key_cols presente\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "# Script 1: Diagnóstico inicial\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# --- Configuración ---\n",
    "FILE_PATH = r\"C:\\Users\\diego\\OneDrive\\9NO CICLO\\ANALÍTICA DE DATOS\\PC1\\Data\\denuncias_2020_2025.csv\"\n",
    "\n",
    "# --- Inicio del Script ---\n",
    "print(\"Iniciando el diagnóstico inicial del archivo CSV (lectura completa)...\")\n",
    "print(f\"Archivo: {FILE_PATH}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Leer TODO el CSV en un DataFrame\n",
    "df = pd.read_csv(\n",
    "    FILE_PATH,\n",
    "    sep=\",\",                # separador fijo\n",
    "    on_bad_lines=\"warn\",\n",
    "    low_memory=False,\n",
    "    na_values=[\"\", \" \", \"  \", \"NA\", \"N/A\", \"na\", \"n/a\", \"NULL\", \"null\", \"Sin dato\", \"SIN DATO\"]\n",
    ")\n",
    "\n",
    "# Normalizar blancos a NaN\n",
    "df = df.replace(r\"^\\s*$\", np.nan, regex=True)\n",
    "\n",
    "# --- 1) Información General y Tipos de Datos ---\n",
    "print(\"## 1) Información General y Tipos de Datos\")\n",
    "print(f\"* Filas totales: {len(df):,}\")\n",
    "print(f\"* Columnas: {df.shape[1]}\")\n",
    "mem_mb = df.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f\"* Uso de memoria aprox.: {mem_mb:.2f} MB\")\n",
    "\n",
    "info_df = pd.DataFrame({\n",
    "    \"Columna\": df.columns,\n",
    "    \"Valores No Nulos\": df.count().values,\n",
    "    \"Tipo de Dato\": [str(t) for t in df.dtypes.values]\n",
    "})\n",
    "\n",
    "# --- 2) Resumen de Valores Faltantes ---\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "missing_df_sorted = (\n",
    "    pd.DataFrame({\n",
    "        \"Columna\": df.columns,\n",
    "        \"Valores Faltantes\": missing_values.values,\n",
    "        \"% Faltante\": missing_percentage.values\n",
    "    })\n",
    "    .query(\"`Valores Faltantes` > 0\")\n",
    "    .sort_values(by=\"% Faltante\", ascending=False)\n",
    ")\n",
    "\n",
    "# --- 3) Estadísticas Descriptivas (Numéricas) ---\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "desc_num = df[num_cols].describe().T if num_cols else pd.DataFrame()\n",
    "\n",
    "# --- 4) Estadísticas Descriptivas (Categóricas) ---\n",
    "obj_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "desc_cat = df[obj_cols].describe().T if obj_cols else pd.DataFrame()\n",
    "\n",
    "# --- 5) Distribución para Categorías Clave (Top 10) ---\n",
    "key_cols = [\"departamento_hecho\", \"tipo_hecho\", \"turno_hecho\", \"ano_hecho\", \"mes_hecho\"]\n",
    "distribuciones = {}\n",
    "for col in key_cols:\n",
    "    if col in df.columns:\n",
    "        distribuciones[col] = (df[col].value_counts(normalize=True, dropna=False).head(10) * 100).round(2)\n",
    "\n",
    "print(\"\\nDiagnóstico listo. Variables útiles en memoria:\")\n",
    "print(\" - df                      → DataFrame completo para seguir trabajando\")\n",
    "print(\" - info_df                 → tabla con no nulos y tipos\")\n",
    "print(\" - missing_df_sorted       → columnas con faltantes (ordenadas por %)\")\n",
    "print(\" - desc_num / desc_cat     → describe() numérico y categórico\")\n",
    "print(\" - distribuciones[col]     → top-10 % para cada columna en key_cols presente\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d796f911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "# Script 2: Manejo de valores faltantes\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# --- Configuración ---\n",
    "COLUMNS_TO_DROP = [\n",
    "    'tipologias_ia',\n",
    "    'cuadra_hecho',\n",
    "    'barrio',\n",
    "    'comisaria',\n",
    "    'departamento',\n",
    "    'provincia',\n",
    "    'distrito',\n",
    "    'indice_priorizacion',\n",
    "    'fecha_inaguracion'\n",
    "]\n",
    "\n",
    "print(\"Iniciando Paso 2: Manejo de valores faltantes (Eliminación de columnas).\")\n",
    "print(f\"Columnas a eliminar: {COLUMNS_TO_DROP}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Usar inplace=True para modificar df directamente\n",
    "df.drop(columns=COLUMNS_TO_DROP, inplace=True, errors='ignore')\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Proceso completado.\")\n",
    "print(f\"Tiempo total de procesamiento: {end_time - start_time:.2f} segundos.\")\n",
    "print(f\"El DataFrame ahora tiene {df.shape[1]} columnas y {df.shape[0]:,} filas.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a89d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "# Script 3: Transformar columnas\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# --- Configuración (igual que tu script) ---\n",
    "COLUMNS_TO_DROP = [\n",
    "    'fecha_hecho',\n",
    "    'hora_hecho'\n",
    "]\n",
    "\n",
    "print(\"Iniciando Paso 3: Transformación de Columnas (Fechas) sobre df en memoria.\")\n",
    "print(f\"Columnas a eliminar: {COLUMNS_TO_DROP}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# 1) Convertir 'fecha_hora_hecho' a datetime (coerce → valores inválidos pasan a NaT)\n",
    "if 'fecha_hora_hecho' in df.columns:\n",
    "    df['fecha_hora_hecho'] = pd.to_datetime(df['fecha_hora_hecho'], errors='coerce')\n",
    "else:\n",
    "    print(\"Aviso: la columna 'fecha_hora_hecho' no existe en df; no se aplicó la conversión.\")\n",
    "\n",
    "# 2) Eliminar columnas de fecha/hora redundantes\n",
    "df.drop(columns=COLUMNS_TO_DROP, inplace=True, errors='ignore')\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Proceso completado.\")\n",
    "print(f\"Tiempo total de procesamiento: {end_time - start_time:.2f} segundos.\")\n",
    "print(f\"Dimensiones actuales de df: {df.shape[0]:,} filas x {df.shape[1]} columnas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2d8b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "# Script 4: Optimizar tipos\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# --- Configuración (idéntica a tu script) ---\n",
    "DTYPE_MAPPING = {\n",
    "    # Enteros a int8\n",
    "    'mes_hecho': 'int8',\n",
    "    'dia_hecho': 'int8',\n",
    "    'id_materia_hecho': 'int8',\n",
    "    'id_dpto_hecho': 'int8',\n",
    "    'solo_denuncia': 'int8',\n",
    "    'estado': 'int8',\n",
    "    # Enteros a int16\n",
    "    'ano_hecho': 'int16',\n",
    "    'id_tipo_hecho': 'int16',\n",
    "    # Floats a float32\n",
    "    'lat': 'float32',\n",
    "    'lon': 'float32',\n",
    "    'lat_hecho': 'float32',\n",
    "    'long_hecho': 'float32',\n",
    "}\n",
    "\n",
    "print(\"Iniciando Paso 4: Optimización de Tipos de Datos (Numéricos) sobre df en memoria.\")\n",
    "print(\"Aplicando las siguientes optimizaciones:\")\n",
    "for col, dtype in DTYPE_MAPPING.items():\n",
    "    print(f\"- Columna '{col}' -> {dtype}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Nota: en tu versión original se parseaba 'fecha_hora_hecho' desde el CSV.\n",
    "# Aquí asumimos que ya quedó en datetime en el Paso 3.\n",
    "\n",
    "for col, dtype in DTYPE_MAPPING.items():\n",
    "    if col in df.columns:\n",
    "        try:\n",
    "            df[col] = df[col].astype(dtype)\n",
    "        except Exception as e:\n",
    "            print(f\"  - Advertencia: No se pudo convertir la columna '{col}'. Error: {e}\")\n",
    "    else:\n",
    "        print(f\"  - Aviso: La columna '{col}' no existe en df; se omite la conversión.\")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Resumen\n",
    "mem_mb = df.memory_usage(deep=True).sum() / 1024**2\n",
    "print(\"\\nProceso completado.\")\n",
    "print(f\"Tiempo total de procesamiento: {end_time - start_time:.2f} segundos.\")\n",
    "print(f\"Dimensiones actuales de df: {df.shape[0]:,} filas x {df.shape[1]} columnas.\")\n",
    "print(f\"Uso de memoria aprox.: {mem_mb:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27d1a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "# Script 5: Verificar tipos\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# --- Configuración ---\n",
    "NUMERIC_COLS = [\n",
    "    'mes_hecho', 'dia_hecho', 'id_tipo_hecho', 'id_materia_hecho',\n",
    "    'ano_hecho', 'id_dpto_hecho', 'solo_denuncia', 'estado',\n",
    "    'lat', 'lon', 'lat_hecho', 'long_hecho'\n",
    "]\n",
    "CATEGORICAL_COLS = [\n",
    "    'departamento_hecho', 'provincia_hecho', 'distrito_hecho',\n",
    "    'tipo_hecho', 'materia_hecho', 'turno_hecho', 'es_delito_x',\n",
    "    'macroregpol_hecho', 'regionpol_hecho', 'estado_coord'\n",
    "]\n",
    "\n",
    "print(\"Iniciando Paso 5: Verificación de Tipos de Datos sobre df en memoria.\")\n",
    "print(\"Esto puede tardar algunos segundos con ~7M filas…\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Filtra a solo columnas existentes para evitar warnings innecesarios\n",
    "num_exist = [c for c in NUMERIC_COLS if c in df.columns]\n",
    "cat_exist = [c for c in CATEGORICAL_COLS if c in df.columns]\n",
    "\n",
    "# --- Rangos (min/max) para numéricas ---\n",
    "if num_exist:\n",
    "    # .agg(['min','max']) es vectorizado y muy eficiente\n",
    "    num_summary = df[num_exist].agg(['min', 'max']).T\n",
    "    num_summary.columns = ['min_real', 'max_real']\n",
    "else:\n",
    "    num_summary = pd.DataFrame(columns=['min_real', 'max_real'])\n",
    "\n",
    "# --- Conteo de valores únicos para categóricas ---\n",
    "if cat_exist:\n",
    "    cat_nunique = df[cat_exist].nunique(dropna=True)\n",
    "else:\n",
    "    cat_nunique = pd.Series(dtype='Int64')\n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "\n",
    "# --- Reporte en consola (mismo estilo que tu script original) ---\n",
    "print(f\"\\nProceso completado en {duration:.2f} segundos. Total de filas analizadas: {len(df):,}\")\n",
    "print(\"\\n--- Rangos de Columnas Numéricas ---\")\n",
    "print(f\"{'Columna':<25} {'Min Real':<20} {'Max Real':<20}\")\n",
    "print(\"-\" * 70)\n",
    "for col in num_summary.index:\n",
    "    mn = num_summary.loc[col, 'min_real']\n",
    "    mx = num_summary.loc[col, 'max_real']\n",
    "    print(f\"{col:<25} {str(mn):<20} {str(mx):<20}\")\n",
    "\n",
    "print(\"\\n--- Conteo de Valores Únicos (Categóricas) ---\")\n",
    "print(f\"{'Columna':<25} {'Valores Únicos':<15}\")\n",
    "print(\"-\" * 45)\n",
    "for col in cat_nunique.index:\n",
    "    print(f\"{col:<25} {int(cat_nunique[col]):<15}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Análisis completado. Usa estos rangos y cardinalidades para validar dtypes/consistencias.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c355e733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import unicodedata\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "# Script 6: Renombrar columnas\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def normalize_column_name(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Normaliza un nombre de columna a formato snake_case.\n",
    "    Ej: 'Año Hecho' -> 'ano_hecho'\n",
    "    \"\"\"\n",
    "    # Transliterar acentos/ñ\n",
    "    nfkd = unicodedata.normalize('NFKD', str(name))\n",
    "    name = ''.join(c for c in nfkd if not unicodedata.combining(c))\n",
    "    # Minúsculas\n",
    "    name = name.lower()\n",
    "    # Reemplazar no alfanuméricos por '_'\n",
    "    name = re.sub(r'[^a-z0-9]+', '_', name)\n",
    "    # Quitar '_' inicial/final\n",
    "    name = name.strip('_')\n",
    "    return name\n",
    "\n",
    "print(\"Iniciando Paso 5: Renombrar y Estandarizar Nombres de Columnas (sobre df en memoria).\")\n",
    "print(\"=\"*80)\n",
    "start_time = time.time()\n",
    "\n",
    "# Construir mapeo old -> new a partir de las columnas actuales del df\n",
    "new_columns = {col: normalize_column_name(col) for col in df.columns}\n",
    "\n",
    "# Mostrar cambios propuestos\n",
    "print(\"Se aplicarán los siguientes cambios en los nombres:\")\n",
    "changed = False\n",
    "for old, new in new_columns.items():\n",
    "    if old != new:\n",
    "        print(f\"- '{old}' -> '{new}'\")\n",
    "        changed = True\n",
    "if not changed:\n",
    "    print(\"(No se encontraron columnas que necesiten cambios)\")\n",
    "\n",
    "# Aplicar renombrado en el DataFrame en memoria\n",
    "df.rename(columns=new_columns, inplace=True)\n",
    "\n",
    "# Aviso si quedaron nombres duplicados tras la normalización\n",
    "dup_cols = df.columns[df.columns.duplicated()].unique().tolist()\n",
    "if dup_cols:\n",
    "    print(\"\\nADVERTENCIA: Se detectaron nombres de columnas duplicados después de normalizar:\")\n",
    "    for c in dup_cols:\n",
    "        print(f\"  - '{c}' aparece {sum(df.columns == c)} veces\")\n",
    "    print(\"Sugerencia: resolver manualmente estas colisiones antes de continuar.\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"\\nProceso completado.\")\n",
    "print(f\"Tiempo total de procesamiento: {end_time - start_time:.2f} segundos.\")\n",
    "print(f\"Dimensiones actuales de df: {df.shape[0]:,} filas x {df.shape[1]} columnas.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3257a143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "# Script 7: Verificar unicidad\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# --- Configuración ---\n",
    "COLUMN_TO_CHECK = 'objectid'\n",
    "\n",
    "print(f\"Iniciando Verificación de Unicidad para la columna '{COLUMN_TO_CHECK}' (sobre df en memoria).\")\n",
    "print(\"=\"*80)\n",
    "start_time = time.time()\n",
    "\n",
    "if COLUMN_TO_CHECK not in df.columns:\n",
    "    raise KeyError(f\"La columna '{COLUMN_TO_CHECK}' no existe en df. Columnas disponibles: {list(df.columns)[:10]}...\")\n",
    "\n",
    "total_rows = len(df)\n",
    "# Contar únicos a nivel global; dropna=False cuenta NaN como una categoría\n",
    "unique_count = df[COLUMN_TO_CHECK].nunique(dropna=False)\n",
    "\n",
    "duration = time.time() - start_time\n",
    "\n",
    "# --- Reporte ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"  Reporte de Verificación de Unicidad\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Proceso completado en {duration:.2f} segundos.\")\n",
    "print(f\"Columna analizada: '{COLUMN_TO_CHECK}'\")\n",
    "print(f\"Total de filas analizadas: {total_rows:,}\")\n",
    "print(f\"Valores únicos encontrados: {unique_count:,}\")\n",
    "\n",
    "print(\"\\n--- Conclusión ---\")\n",
    "if total_rows == unique_count:\n",
    "    print(\"(OK) ¡Confirmado! La columna es un identificador único. No hay duplicados.\")\n",
    "else:\n",
    "    calculated_duplicates = total_rows - unique_count\n",
    "    print(f\"(ERROR) ¡Atención! La columna NO es un identificador único.\")\n",
    "    print(f\"   Se encontraron {calculated_duplicates:,} filas duplicadas basadas en '{COLUMN_TO_CHECK}'.\")\n",
    "\n",
    "    # (Opcional) muestra una vista rápida de IDs duplicados y sus conteos\n",
    "    dup_counts = df[COLUMN_TO_CHECK].value_counts(dropna=False)\n",
    "    top_dups = dup_counts[dup_counts > 1].head(10)\n",
    "    if not top_dups.empty:\n",
    "        print(\"\\nEjemplos de IDs duplicados (top 10):\")\n",
    "        for idx, cnt in top_dups.items():\n",
    "            print(f\"  - {idx}: {cnt} veces\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cb3746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "# Script 8: Verificar categorías\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# --- Configuración ---\n",
    "CATEGORICAL_COLS = [\n",
    "    'departamento_hecho', 'provincia_hecho', 'distrito_hecho',\n",
    "    'tipo_hecho', 'materia_hecho', 'turno_hecho', 'es_delito_x',\n",
    "    'macroregpol_hecho', 'regionpol_hecho', 'estado_coord'\n",
    "]\n",
    "\n",
    "print(\"Iniciando Verificación de Valores Únicos para Columnas Categóricas (sobre df en memoria).\")\n",
    "print(\"Columnas objetivo:\", CATEGORICAL_COLS)\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Filtrar a las columnas que realmente existen en df\n",
    "cat_exist = [c for c in CATEGORICAL_COLS if c in df.columns]\n",
    "missing = [c for c in CATEGORICAL_COLS if c not in df.columns]\n",
    "if missing:\n",
    "    print(\"Aviso: estas columnas no existen en df y serán omitidas:\", missing)\n",
    "\n",
    "# Construir diccionario de valores únicos\n",
    "final_uniques_dict = {}\n",
    "for col in cat_exist:\n",
    "    # dropna() para ignorar NaN; sorted para salida consistente\n",
    "    uniques = df[col].dropna().unique()\n",
    "    try:\n",
    "        # Si viene como dtype 'category', .unique() ya es eficiente\n",
    "        uniques_list = sorted([str(x) for x in uniques])\n",
    "    except Exception:\n",
    "        # Fallback robusto\n",
    "        uniques_list = sorted(map(str, pd.Series(uniques).astype(str).tolist()))\n",
    "    final_uniques_dict[col] = uniques_list\n",
    "\n",
    "duration = time.time() - start_time\n",
    "\n",
    "# --- Reporte en consola (legible) ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"  Diccionario de Valores Únicos por Columna Categórica\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Proceso completado en {duration:.2f} segundos.\")\n",
    "for col, values in final_uniques_dict.items():\n",
    "    print(f\"\\n--- Columna: '{col}' ({len(values)} valores únicos) ---\")\n",
    "    if len(values) > 20:\n",
    "        print(values[:10], \"...\")\n",
    "    else:\n",
    "        print(values)\n",
    "\n",
    "print(\"\\nNota: El diccionario completo quedó en la variable `final_uniques_dict` para uso posterior.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff1a079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "# Script 9: Codificación y Binning\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "\n",
    "print(\"Iniciando Paso 7: Codificación y Binning (sobre df en memoria).\")\n",
    "print(\"=\"*80)\n",
    "start_time = time.time()\n",
    "\n",
    "# --- 1) Limpieza para 'estado_coord' ---\n",
    "estado_coord_replace_map = {\n",
    "    'SIN COORDENADA XX': 'SIN COORDENADA',\n",
    "    'SIN COORDENADA YY': 'SIN COORDENADA'\n",
    "}\n",
    "\n",
    "# --- 2) Codificación Ordinal para 'turno_hecho' (asume lower-case) ---\n",
    "turno_hecho_encoding_map = {\n",
    "    'madrugada': 0,\n",
    "    'mañana': 1,\n",
    "    'tarde': 2,\n",
    "    'noche': 3\n",
    "}\n",
    "\n",
    "# --- 3) Binning para la hora del día desde 'fecha_hora_hecho' ---\n",
    "hour_bins   = [-1, 5, 11, 17, 23]\n",
    "hour_labels = ['Madrugada', 'Mañana', 'Tarde', 'Noche']\n",
    "\n",
    "# --- 4) Codificación binaria para 'tiene_coordenada' a partir de 'estado_coord' ---\n",
    "tiene_coordenada_map = {\n",
    "    'CON COORDENADA': 1,\n",
    "    'SIN COORDENADA': 0\n",
    "}\n",
    "\n",
    "# --- Diccionario de Datos para documentación (en memoria) ---\n",
    "data_dictionary = {\n",
    "    \"estado_coord_cleaning\": {\n",
    "        \"description\": \"Se unificaron valores en la columna 'estado_coord'.\",\n",
    "        \"mapping\": estado_coord_replace_map\n",
    "    },\n",
    "    \"turno_hecho_encoding\": {\n",
    "        \"description\": \"Codificación ordinal de la columna 'turno_hecho'.\",\n",
    "        \"mapping\": {\"0\": \"madrugada\", \"1\": \"mañana\", \"2\": \"tarde\", \"3\": \"noche\"}\n",
    "    },\n",
    "    \"periodo_dia_binning\": {\n",
    "        \"description\": \"Nueva columna creada agrupando la hora de 'fecha_hora_hecho'.\",\n",
    "        \"bins\": {\"0-5\": \"Madrugada\", \"6-11\": \"Mañana\", \"12-17\": \"Tarde\", \"18-23\": \"Noche\"}\n",
    "    },\n",
    "    \"tiene_coordenada_encoding\": {\n",
    "        \"description\": \"Codificación binaria (0/1) creada a partir de 'estado_coord' limpio.\",\n",
    "        \"mapping\": tiene_coordenada_map\n",
    "    }\n",
    "}\n",
    "\n",
    "# ---------- Aplicación sobre df ----------\n",
    "# 1) Limpiar 'estado_coord'\n",
    "if 'estado_coord' in df.columns:\n",
    "    df['estado_coord'] = df['estado_coord'].replace(estado_coord_replace_map)\n",
    "else:\n",
    "    print(\"Aviso: 'estado_coord' no existe en df; se omite su limpieza.\")\n",
    "\n",
    "# 2) Codificar 'turno_hecho' → 'turno_hecho_cod'\n",
    "if 'turno_hecho' in df.columns:\n",
    "    # Asegura lower-case si no lo hiciste antes\n",
    "    df['turno_hecho_cod'] = (\n",
    "        df['turno_hecho']\n",
    "        .astype('string')\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .map(turno_hecho_encoding_map)\n",
    "        .astype('Int8')  # permite NaN\n",
    "    )\n",
    "else:\n",
    "    print(\"Aviso: 'turno_hecho' no existe en df; no se creó 'turno_hecho_cod'.\")\n",
    "\n",
    "# 3) Binning por hora de 'fecha_hora_hecho' → 'periodo_dia'\n",
    "if 'fecha_hora_hecho' in df.columns:\n",
    "    # Asegura tipo datetime (Paso 3 ya lo hacía, esto es por si acaso)\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df['fecha_hora_hecho']):\n",
    "        df['fecha_hora_hecho'] = pd.to_datetime(df['fecha_hora_hecho'], errors='coerce')\n",
    "    horas = df['fecha_hora_hecho'].dt.hour  # NaT → NaN\n",
    "    df['periodo_dia'] = pd.cut(horas, bins=hour_bins, labels=hour_labels, right=True)\n",
    "else:\n",
    "    print(\"Aviso: 'fecha_hora_hecho' no existe en df; no se creó 'periodo_dia'.\")\n",
    "\n",
    "# 4) 'tiene_coordenada' (0/1) a partir de 'estado_coord' limpio\n",
    "if 'estado_coord' in df.columns:\n",
    "    df['tiene_coordenada'] = (\n",
    "        df['estado_coord']\n",
    "        .astype('string')\n",
    "        .str.strip()\n",
    "        .str.upper()\n",
    "        .map(tiene_coordenada_map)\n",
    "        .astype('Int8')  # permite NaN cuando no machea\n",
    "    )\n",
    "else:\n",
    "    print(\"Aviso: 'estado_coord' no existe en df; no se creó 'tiene_coordenada'.\")\n",
    "\n",
    "dur = time.time() - start_time\n",
    "print(\"\\nProceso completado.\")\n",
    "print(f\"Tiempo total de procesamiento: {dur:.2f} segundos.\")\n",
    "print(f\"Dimensiones actuales de df: {df.shape[0]:,} filas x {df.shape[1]} columnas.\")\n",
    "print(\"Variables nuevas/afectadas: 'estado_coord' (limpia), 'turno_hecho_cod', 'periodo_dia', 'tiene_coordenada'.\")\n",
    "print(\"Diccionario de datos disponible en la variable `data_dictionary`.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3884360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "# Script 10: Filtrar atípicos\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# --- Configuración ---\n",
    "OUTPUT_FILE = r\"C:\\Users\\diego\\OneDrive\\9NO CICLO\\ANALÍTICA DE DATOS\\PC1\\ENTREGABLES\\data_denuncias.csv\"\n",
    "\n",
    "# Límites geográficos de Perú (con margen)\n",
    "LAT_MIN, LAT_MAX = -18.4, 0\n",
    "LON_MIN, LON_MAX = -81.4, -68.6\n",
    "\n",
    "print(\"Iniciando Paso 8: Filtrado de Valores Atípicos (Geográficos) sobre df en memoria.\")\n",
    "print(f\"Archivo de salida: {OUTPUT_FILE}\")\n",
    "print(f\"Límites de Latitud: {LAT_MIN} a {LAT_MAX}\")\n",
    "print(f\"Límites de Longitud: {LON_MIN} a {LON_MAX}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time = time.time()\n",
    "rows_before = len(df)\n",
    "\n",
    "# --- Filtrado ---\n",
    "if 'lat' in df.columns and 'lon' in df.columns:\n",
    "    df = df[df['lat'].between(LAT_MIN, LAT_MAX) & df['lon'].between(LON_MIN, LON_MAX)]\n",
    "else:\n",
    "    raise KeyError(\"No se encontraron las columnas 'lat' y 'lon' en df.\")\n",
    "\n",
    "rows_after = len(df)\n",
    "outlier_rows = rows_before - rows_after\n",
    "\n",
    "# --- Guardar a CSV ---\n",
    "df.to_csv(OUTPUT_FILE, index=False, encoding=\"utf-8\")\n",
    "\n",
    "duration = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Proceso de filtrado completado.\")\n",
    "print(f\"Total de filas procesadas: {rows_before:,}\")\n",
    "print(f\"Filas eliminadas como outliers: {outlier_rows:,}\")\n",
    "print(f\"Filas finales en df: {rows_after:,}\")\n",
    "print(f\"Archivo final limpio guardado en:\\n{OUTPUT_FILE}\")\n",
    "print(f\"Tiempo total de procesamiento: {duration:.2f} segundos.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
